version: '3.8'

x-airflow-common: &airflow-common
  image: apache/airflow:2.9.0
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: SequentialExecutor
    AIRFLOW__CORE__LOAD_EXAMPLES: "False"
    AIRFLOW__CORE__FERNET_KEY: 'UwUdzvRfbjRDgcZUuULaaWo1kBACmq6RMNkU1IX25bM='
    AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
    AIRFLOW__WEBSERVER__SECRET_KEY: 'UwUdzvRfbjRDgcZUuULaaWo1kBACmq6RMNkU1IX25bM='
    _AIRFLOW_WWW_USER_USERNAME: admin
    _AIRFLOW_WWW_USER_PASSWORD: admin
    _PIP_ADDITIONAL_REQUIREMENTS: 'six confluent-kafka==2.3.0 pymongo==4.6.0'
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - ./airflow-db:/opt/airflow
  user: "50000:0"

services:
  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    command: >
      bash -c "
      airflow db init &&
      airflow users create --username admin --firstname Admin --lastname Admin --role Admin --email admin@example.com --password admin
      "
    restart: "no"

  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    restart: always
    ports:
      - "8080:8080"
    command: webserver
    depends_on:
      - airflow-init
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    restart: always
    command: scheduler
    depends_on:
      - airflow-init
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 3
